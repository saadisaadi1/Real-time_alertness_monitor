C:\Users\USER\PyCharmMiscProject\.venv\Scripts\python.exe C:\Users\USER\PyCharmMiscProject\model1.py 

==================================================
GPU Configuration
==================================================
✓ CUDA is available!
GPU Device: NVIDIA GeForce RTX 3050
CUDA Version: 11.8
GPU Memory: 8.00 GB
Using device: cuda
==================================================

==================================================
Loading Preprocessed DAiSEE Dataset...
==================================================
Loaded preprocessed dataset from: C:\Users\USER\PyCharmMiscProject\processed_daisee\processed_dataset.csv
Total images: 90284
Train: 55003
Validation: 17296
Test: 17985

==================================================
Creating 80/10/10 Train/Val/Test Split...
==================================================

Dataset split complete:
  Total images: 90284
  Train: 72227 (80.0%)
  Val: 9028 (10.0%)
  Test: 9029 (10.0%)

Engagement label distribution:

  Train:
    Level 0: 490 samples (0.7%)
    Level 1: 3698 samples (5.1%)
    Level 2: 35613 samples (49.3%)
    Level 3: 32426 samples (44.9%)

  Val:
    Level 0: 57 samples (0.6%)
    Level 1: 444 samples (4.9%)
    Level 2: 4478 samples (49.6%)
    Level 3: 4049 samples (44.8%)

  Test:
    Level 0: 63 samples (0.7%)
    Level 1: 456 samples (5.1%)
    Level 2: 4444 samples (49.2%)
    Level 3: 4066 samples (45.0%)

DataLoaders created:
  Train batches: 2258
  Val batches: 283
  Test batches: 283

==================================================
Building Model...
==================================================

==================================================
Starting Training...
==================================================

Epoch 1/10
------------------------------------------------------------
GPU Memory Allocated: 42.69 MB
GPU Memory Cached: 64.00 MB
  Batch 0/2258 | Loss: 1.6728 | Acc: 9.38%
  Batch 50/2258 | Loss: 0.9508 | Acc: 49.82%
  Batch 100/2258 | Loss: 0.9739 | Acc: 51.61%
  Batch 150/2258 | Loss: 0.7958 | Acc: 52.36%
  Batch 200/2258 | Loss: 0.6663 | Acc: 53.33%
  Batch 250/2258 | Loss: 0.7765 | Acc: 53.56%
  Batch 300/2258 | Loss: 0.7590 | Acc: 54.11%
  Batch 350/2258 | Loss: 0.9688 | Acc: 54.10%
  Batch 400/2258 | Loss: 1.0273 | Acc: 54.27%
  Batch 450/2258 | Loss: 0.9068 | Acc: 54.27%
  Batch 500/2258 | Loss: 0.9665 | Acc: 54.31%
  Batch 550/2258 | Loss: 0.7209 | Acc: 54.41%
  Batch 600/2258 | Loss: 0.9116 | Acc: 54.40%
  Batch 650/2258 | Loss: 0.9056 | Acc: 54.59%
  Batch 700/2258 | Loss: 0.7403 | Acc: 54.81%
  Batch 750/2258 | Loss: 1.0128 | Acc: 55.06%
  Batch 800/2258 | Loss: 0.5988 | Acc: 55.17%
  Batch 850/2258 | Loss: 0.8472 | Acc: 55.29%
  Batch 900/2258 | Loss: 1.0013 | Acc: 55.58%
  Batch 950/2258 | Loss: 0.7906 | Acc: 55.64%
  Batch 1000/2258 | Loss: 0.8745 | Acc: 55.82%
  Batch 1050/2258 | Loss: 0.7869 | Acc: 55.92%
  Batch 1100/2258 | Loss: 1.1571 | Acc: 56.05%
  Batch 1150/2258 | Loss: 0.8237 | Acc: 56.18%
  Batch 1200/2258 | Loss: 0.8787 | Acc: 56.27%
  Batch 1250/2258 | Loss: 0.7534 | Acc: 56.35%
  Batch 1300/2258 | Loss: 0.8299 | Acc: 56.37%
  Batch 1350/2258 | Loss: 0.8127 | Acc: 56.50%
  Batch 1400/2258 | Loss: 0.9203 | Acc: 56.54%
  Batch 1450/2258 | Loss: 0.8614 | Acc: 56.53%
  Batch 1500/2258 | Loss: 0.9506 | Acc: 56.67%
  Batch 1550/2258 | Loss: 0.8956 | Acc: 56.71%
  Batch 1600/2258 | Loss: 0.7914 | Acc: 56.76%
  Batch 1650/2258 | Loss: 0.5593 | Acc: 56.87%
  Batch 1700/2258 | Loss: 0.7646 | Acc: 56.96%
  Batch 1750/2258 | Loss: 0.8249 | Acc: 57.09%
  Batch 1800/2258 | Loss: 0.9050 | Acc: 57.09%
  Batch 1850/2258 | Loss: 0.9360 | Acc: 57.11%
  Batch 1900/2258 | Loss: 0.6952 | Acc: 57.24%
  Batch 1950/2258 | Loss: 0.6773 | Acc: 57.23%
  Batch 2000/2258 | Loss: 0.7983 | Acc: 57.24%
  Batch 2050/2258 | Loss: 0.7776 | Acc: 57.26%
  Batch 2100/2258 | Loss: 0.7456 | Acc: 57.32%
  Batch 2150/2258 | Loss: 0.8613 | Acc: 57.36%
  Batch 2200/2258 | Loss: 0.7615 | Acc: 57.42%
  Batch 2250/2258 | Loss: 0.8054 | Acc: 57.47%

Epoch 1 Summary:
  Train Loss: 0.8579 | Train Acc: 57.49%
  Val Loss: 0.8165 | Val Acc: 61.56%
  Time: 244.6s (4.1 min)
  ✓ New best model saved! (Val Acc: 61.56%)
  Validation label distribution:
    Low: True=57, Predicted=0
    Medium-Low: True=444, Predicted=0
    Medium-High: True=4478, Predicted=5818
    High: True=4049, Predicted=3210

Epoch 2/10
------------------------------------------------------------
GPU Memory Allocated: 192.74 MB
GPU Memory Cached: 308.00 MB
  Batch 0/2258 | Loss: 0.7391 | Acc: 68.75%
  Batch 50/2258 | Loss: 0.8044 | Acc: 60.85%
  Batch 100/2258 | Loss: 1.0682 | Acc: 60.80%
  Batch 150/2258 | Loss: 0.8780 | Acc: 60.62%
  Batch 200/2258 | Loss: 0.7952 | Acc: 60.73%
  Batch 250/2258 | Loss: 0.7246 | Acc: 60.69%
  Batch 300/2258 | Loss: 0.7970 | Acc: 60.67%
  Batch 350/2258 | Loss: 0.8063 | Acc: 60.22%
  Batch 400/2258 | Loss: 0.9941 | Acc: 60.03%
  Batch 450/2258 | Loss: 0.9436 | Acc: 60.17%
  Batch 500/2258 | Loss: 0.7867 | Acc: 59.94%
  Batch 550/2258 | Loss: 0.7156 | Acc: 59.93%
  Batch 600/2258 | Loss: 0.7811 | Acc: 59.78%
  Batch 650/2258 | Loss: 0.8078 | Acc: 59.71%
  Batch 700/2258 | Loss: 0.6451 | Acc: 59.67%
  Batch 750/2258 | Loss: 0.7397 | Acc: 59.64%
  Batch 800/2258 | Loss: 0.9892 | Acc: 59.60%
  Batch 850/2258 | Loss: 0.9315 | Acc: 59.47%
  Batch 900/2258 | Loss: 0.8924 | Acc: 59.54%
  Batch 950/2258 | Loss: 0.7009 | Acc: 59.59%
  Batch 1000/2258 | Loss: 0.9808 | Acc: 59.50%
  Batch 1050/2258 | Loss: 0.7449 | Acc: 59.43%
  Batch 1100/2258 | Loss: 0.9568 | Acc: 59.52%
  Batch 1150/2258 | Loss: 1.0248 | Acc: 59.48%
  Batch 1200/2258 | Loss: 0.7215 | Acc: 59.41%
  Batch 1250/2258 | Loss: 0.8722 | Acc: 59.48%
  Batch 1300/2258 | Loss: 1.1012 | Acc: 59.41%
  Batch 1350/2258 | Loss: 0.6847 | Acc: 59.53%
  Batch 1400/2258 | Loss: 0.9104 | Acc: 59.54%
  Batch 1450/2258 | Loss: 0.7746 | Acc: 59.54%
  Batch 1500/2258 | Loss: 0.9192 | Acc: 59.67%
  Batch 1550/2258 | Loss: 0.8984 | Acc: 59.69%
  Batch 1600/2258 | Loss: 0.7000 | Acc: 59.74%
  Batch 1650/2258 | Loss: 0.7600 | Acc: 59.81%
  Batch 1700/2258 | Loss: 0.7292 | Acc: 59.84%
  Batch 1750/2258 | Loss: 0.9716 | Acc: 59.88%
  Batch 1800/2258 | Loss: 0.8136 | Acc: 59.93%
  Batch 1850/2258 | Loss: 0.9412 | Acc: 59.96%
  Batch 1900/2258 | Loss: 0.7051 | Acc: 60.01%
  Batch 1950/2258 | Loss: 0.7770 | Acc: 60.01%
  Batch 2000/2258 | Loss: 0.8055 | Acc: 60.03%
  Batch 2050/2258 | Loss: 0.7992 | Acc: 60.05%
  Batch 2100/2258 | Loss: 0.6436 | Acc: 60.03%
  Batch 2150/2258 | Loss: 0.7520 | Acc: 60.07%
  Batch 2200/2258 | Loss: 0.9583 | Acc: 60.09%
  Batch 2250/2258 | Loss: 0.8796 | Acc: 60.06%

Epoch 2 Summary:
  Train Loss: 0.8192 | Train Acc: 60.07%
  Val Loss: 0.7901 | Val Acc: 62.51%
  Time: 230.6s (3.8 min)
  ✓ New best model saved! (Val Acc: 62.51%)
  Validation label distribution:
    Low: True=57, Predicted=0
    Medium-Low: True=444, Predicted=87
    Medium-High: True=4478, Predicted=5317
    High: True=4049, Predicted=3624

Epoch 3/10
------------------------------------------------------------
GPU Memory Allocated: 192.51 MB
GPU Memory Cached: 350.00 MB
  Batch 0/2258 | Loss: 0.6524 | Acc: 68.75%
  Batch 50/2258 | Loss: 0.7947 | Acc: 61.64%
  Batch 100/2258 | Loss: 0.7779 | Acc: 61.60%
  Batch 150/2258 | Loss: 0.8148 | Acc: 61.65%
  Batch 200/2258 | Loss: 0.8876 | Acc: 61.75%
  Batch 250/2258 | Loss: 0.9135 | Acc: 61.77%
  Batch 300/2258 | Loss: 0.6842 | Acc: 61.79%
  Batch 350/2258 | Loss: 0.5940 | Acc: 61.75%
  Batch 400/2258 | Loss: 0.7526 | Acc: 61.59%
  Batch 450/2258 | Loss: 0.7974 | Acc: 61.49%
  Batch 500/2258 | Loss: 0.7113 | Acc: 61.39%
  Batch 550/2258 | Loss: 0.8629 | Acc: 61.42%
  Batch 600/2258 | Loss: 0.7342 | Acc: 61.49%
  Batch 650/2258 | Loss: 0.8075 | Acc: 61.63%
  Batch 700/2258 | Loss: 0.8383 | Acc: 61.70%
  Batch 750/2258 | Loss: 0.6775 | Acc: 61.72%
  Batch 800/2258 | Loss: 0.7747 | Acc: 61.61%
  Batch 850/2258 | Loss: 0.9212 | Acc: 61.60%
  Batch 900/2258 | Loss: 0.6421 | Acc: 61.58%
  Batch 950/2258 | Loss: 0.8140 | Acc: 61.62%
  Batch 1000/2258 | Loss: 0.7136 | Acc: 61.68%
  Batch 1050/2258 | Loss: 0.7984 | Acc: 61.65%
  Batch 1100/2258 | Loss: 0.7629 | Acc: 61.65%
  Batch 1150/2258 | Loss: 0.7579 | Acc: 61.59%
  Batch 1200/2258 | Loss: 0.9344 | Acc: 61.57%
  Batch 1250/2258 | Loss: 0.5862 | Acc: 61.59%
  Batch 1300/2258 | Loss: 0.7733 | Acc: 61.54%
  Batch 1350/2258 | Loss: 0.8049 | Acc: 61.51%
  Batch 1400/2258 | Loss: 1.0428 | Acc: 61.56%
  Batch 1450/2258 | Loss: 0.6193 | Acc: 61.54%
  Batch 1500/2258 | Loss: 0.9550 | Acc: 61.51%
  Batch 1550/2258 | Loss: 0.9145 | Acc: 61.50%
  Batch 1600/2258 | Loss: 0.7202 | Acc: 61.56%
  Batch 1650/2258 | Loss: 0.6467 | Acc: 61.65%
  Batch 1700/2258 | Loss: 0.6620 | Acc: 61.63%
  Batch 1750/2258 | Loss: 1.1596 | Acc: 61.57%
  Batch 1800/2258 | Loss: 0.7006 | Acc: 61.52%
  Batch 1850/2258 | Loss: 0.6748 | Acc: 61.54%
  Batch 1900/2258 | Loss: 0.7705 | Acc: 61.59%
  Batch 1950/2258 | Loss: 0.7593 | Acc: 61.63%
  Batch 2000/2258 | Loss: 0.7931 | Acc: 61.68%
  Batch 2050/2258 | Loss: 0.8433 | Acc: 61.65%
  Batch 2100/2258 | Loss: 1.2089 | Acc: 61.71%
  Batch 2150/2258 | Loss: 0.7813 | Acc: 61.75%
  Batch 2200/2258 | Loss: 0.9588 | Acc: 61.77%
  Batch 2250/2258 | Loss: 0.7338 | Acc: 61.78%

Epoch 3 Summary:
  Train Loss: 0.7951 | Train Acc: 61.77%
  Val Loss: 0.7792 | Val Acc: 62.98%
  Time: 231.2s (3.9 min)
  ✓ New best model saved! (Val Acc: 62.98%)
  Validation label distribution:
    Low: True=57, Predicted=23
    Medium-Low: True=444, Predicted=14
    Medium-High: True=4478, Predicted=4500
    High: True=4049, Predicted=4491

Epoch 4/10
------------------------------------------------------------
GPU Memory Allocated: 193.49 MB
GPU Memory Cached: 336.00 MB
  Batch 0/2258 | Loss: 0.6493 | Acc: 68.75%
  Batch 50/2258 | Loss: 0.8296 | Acc: 62.56%
  Batch 100/2258 | Loss: 0.9033 | Acc: 62.75%
  Batch 150/2258 | Loss: 0.8039 | Acc: 63.72%
  Batch 200/2258 | Loss: 0.7311 | Acc: 63.62%
  Batch 250/2258 | Loss: 0.8116 | Acc: 63.33%
  Batch 300/2258 | Loss: 0.8847 | Acc: 63.66%
  Batch 350/2258 | Loss: 0.7289 | Acc: 63.36%
  Batch 400/2258 | Loss: 1.0765 | Acc: 63.54%
  Batch 450/2258 | Loss: 0.7908 | Acc: 63.41%
  Batch 500/2258 | Loss: 0.7989 | Acc: 63.41%
  Batch 550/2258 | Loss: 0.6400 | Acc: 63.51%
  Batch 600/2258 | Loss: 0.7031 | Acc: 63.40%
  Batch 650/2258 | Loss: 0.7848 | Acc: 63.28%
  Batch 700/2258 | Loss: 0.8426 | Acc: 63.26%
  Batch 750/2258 | Loss: 0.8551 | Acc: 63.16%
  Batch 800/2258 | Loss: 0.8264 | Acc: 63.10%
  Batch 850/2258 | Loss: 0.6254 | Acc: 63.08%
  Batch 900/2258 | Loss: 0.8625 | Acc: 62.98%
  Batch 950/2258 | Loss: 1.0347 | Acc: 62.94%
  Batch 1000/2258 | Loss: 0.7938 | Acc: 62.93%
  Batch 1050/2258 | Loss: 0.6955 | Acc: 62.99%
  Batch 1100/2258 | Loss: 0.6331 | Acc: 63.02%
  Batch 1150/2258 | Loss: 0.6396 | Acc: 62.98%
  Batch 1200/2258 | Loss: 0.6439 | Acc: 62.95%
  Batch 1250/2258 | Loss: 0.6605 | Acc: 62.88%
  Batch 1300/2258 | Loss: 0.6741 | Acc: 62.94%
  Batch 1350/2258 | Loss: 0.7137 | Acc: 62.96%
  Batch 1400/2258 | Loss: 0.6892 | Acc: 62.95%
  Batch 1450/2258 | Loss: 0.9291 | Acc: 63.03%
  Batch 1500/2258 | Loss: 0.7400 | Acc: 63.10%
  Batch 1550/2258 | Loss: 0.8681 | Acc: 63.10%
  Batch 1600/2258 | Loss: 0.6528 | Acc: 63.15%
  Batch 1650/2258 | Loss: 0.9871 | Acc: 63.19%
  Batch 1700/2258 | Loss: 0.7599 | Acc: 63.17%
  Batch 1750/2258 | Loss: 0.7987 | Acc: 63.19%
  Batch 1800/2258 | Loss: 0.7363 | Acc: 63.21%
  Batch 1850/2258 | Loss: 0.7414 | Acc: 63.21%
  Batch 1900/2258 | Loss: 0.9096 | Acc: 63.25%
  Batch 1950/2258 | Loss: 0.6456 | Acc: 63.29%
  Batch 2000/2258 | Loss: 0.6759 | Acc: 63.31%
  Batch 2050/2258 | Loss: 0.6124 | Acc: 63.36%
  Batch 2100/2258 | Loss: 0.7939 | Acc: 63.36%
  Batch 2150/2258 | Loss: 0.8288 | Acc: 63.38%
  Batch 2200/2258 | Loss: 0.8666 | Acc: 63.43%
  Batch 2250/2258 | Loss: 0.7170 | Acc: 63.41%

Epoch 4 Summary:
  Train Loss: 0.7639 | Train Acc: 63.41%
  Val Loss: 0.7445 | Val Acc: 64.90%
  Time: 230.8s (3.8 min)
  ✓ New best model saved! (Val Acc: 64.90%)
  Validation label distribution:
    Low: True=57, Predicted=11
    Medium-Low: True=444, Predicted=69
    Medium-High: True=4478, Predicted=4820
    High: True=4049, Predicted=4128

Epoch 5/10
------------------------------------------------------------
GPU Memory Allocated: 192.51 MB
GPU Memory Cached: 350.00 MB
  Batch 0/2258 | Loss: 0.7094 | Acc: 62.50%
  Batch 50/2258 | Loss: 0.7430 | Acc: 62.93%
  Batch 100/2258 | Loss: 0.8192 | Acc: 63.99%
  Batch 150/2258 | Loss: 0.5679 | Acc: 64.59%
  Batch 200/2258 | Loss: 0.5523 | Acc: 65.07%
  Batch 250/2258 | Loss: 0.6909 | Acc: 64.94%
  Batch 300/2258 | Loss: 0.7064 | Acc: 64.99%
  Batch 350/2258 | Loss: 0.9537 | Acc: 64.91%
  Batch 400/2258 | Loss: 0.7590 | Acc: 64.65%
  Batch 450/2258 | Loss: 0.8060 | Acc: 64.92%
  Batch 500/2258 | Loss: 0.8801 | Acc: 64.95%
  Batch 550/2258 | Loss: 0.7634 | Acc: 64.85%
  Batch 600/2258 | Loss: 0.5221 | Acc: 64.78%
  Batch 650/2258 | Loss: 0.7600 | Acc: 64.74%
  Batch 700/2258 | Loss: 1.2056 | Acc: 64.95%
  Batch 750/2258 | Loss: 0.7671 | Acc: 65.01%
  Batch 800/2258 | Loss: 0.7310 | Acc: 64.93%
  Batch 850/2258 | Loss: 0.8852 | Acc: 64.97%
  Batch 900/2258 | Loss: 0.8147 | Acc: 64.97%
  Batch 950/2258 | Loss: 0.7202 | Acc: 65.03%
  Batch 1000/2258 | Loss: 0.7662 | Acc: 65.09%
  Batch 1050/2258 | Loss: 0.8849 | Acc: 65.05%
  Batch 1100/2258 | Loss: 0.7395 | Acc: 65.08%
  Batch 1150/2258 | Loss: 0.7101 | Acc: 65.20%
  Batch 1200/2258 | Loss: 0.8355 | Acc: 65.17%
  Batch 1250/2258 | Loss: 0.6022 | Acc: 65.23%
  Batch 1300/2258 | Loss: 0.9074 | Acc: 65.29%
  Batch 1350/2258 | Loss: 0.6789 | Acc: 65.28%
  Batch 1400/2258 | Loss: 0.8783 | Acc: 65.40%
  Batch 1450/2258 | Loss: 0.7903 | Acc: 65.32%
  Batch 1500/2258 | Loss: 0.6598 | Acc: 65.38%
  Batch 1550/2258 | Loss: 0.6722 | Acc: 65.34%
  Batch 1600/2258 | Loss: 0.5300 | Acc: 65.35%
  Batch 1650/2258 | Loss: 0.9083 | Acc: 65.27%
  Batch 1700/2258 | Loss: 0.6470 | Acc: 65.33%
  Batch 1750/2258 | Loss: 0.5841 | Acc: 65.39%
  Batch 1800/2258 | Loss: 0.7276 | Acc: 65.43%
  Batch 1850/2258 | Loss: 0.6942 | Acc: 65.38%
  Batch 1900/2258 | Loss: 0.8399 | Acc: 65.40%
  Batch 1950/2258 | Loss: 0.6141 | Acc: 65.43%
  Batch 2000/2258 | Loss: 0.6934 | Acc: 65.46%
  Batch 2050/2258 | Loss: 0.5623 | Acc: 65.45%
  Batch 2100/2258 | Loss: 0.7904 | Acc: 65.47%
  Batch 2150/2258 | Loss: 0.5915 | Acc: 65.46%
  Batch 2200/2258 | Loss: 0.7771 | Acc: 65.50%
  Batch 2250/2258 | Loss: 0.6737 | Acc: 65.53%

Epoch 5 Summary:
  Train Loss: 0.7234 | Train Acc: 65.54%
  Val Loss: 0.7110 | Val Acc: 66.83%
  Time: 230.7s (3.8 min)
  ✓ New best model saved! (Val Acc: 66.83%)
  Validation label distribution:
    Low: True=57, Predicted=63
    Medium-Low: True=444, Predicted=226
    Medium-High: True=4478, Predicted=5035
    High: True=4049, Predicted=3704

Epoch 6/10
------------------------------------------------------------
GPU Memory Allocated: 193.49 MB
GPU Memory Cached: 336.00 MB
  Batch 0/2258 | Loss: 0.7185 | Acc: 65.62%
  Batch 50/2258 | Loss: 0.8027 | Acc: 69.85%
  Batch 100/2258 | Loss: 0.6370 | Acc: 68.32%
  Batch 150/2258 | Loss: 0.6688 | Acc: 68.34%
  Batch 200/2258 | Loss: 0.8367 | Acc: 69.03%
  Batch 250/2258 | Loss: 0.5529 | Acc: 69.24%
  Batch 300/2258 | Loss: 0.6160 | Acc: 69.36%
  Batch 350/2258 | Loss: 0.5389 | Acc: 69.75%
  Batch 400/2258 | Loss: 0.4845 | Acc: 69.77%
  Batch 450/2258 | Loss: 0.8047 | Acc: 69.98%
  Batch 500/2258 | Loss: 0.6482 | Acc: 70.08%
  Batch 550/2258 | Loss: 0.5546 | Acc: 70.01%
  Batch 600/2258 | Loss: 0.4673 | Acc: 70.09%
  Batch 650/2258 | Loss: 0.5949 | Acc: 70.02%
  Batch 700/2258 | Loss: 0.5965 | Acc: 70.13%
  Batch 750/2258 | Loss: 0.5643 | Acc: 70.15%
  Batch 800/2258 | Loss: 0.5349 | Acc: 70.22%
  Batch 850/2258 | Loss: 0.5063 | Acc: 70.22%
  Batch 900/2258 | Loss: 0.4328 | Acc: 70.27%
  Batch 950/2258 | Loss: 0.6864 | Acc: 70.33%
  Batch 1000/2258 | Loss: 0.6306 | Acc: 70.37%
  Batch 1050/2258 | Loss: 0.6505 | Acc: 70.45%
  Batch 1100/2258 | Loss: 0.6228 | Acc: 70.56%
  Batch 1150/2258 | Loss: 0.8083 | Acc: 70.62%
  Batch 1200/2258 | Loss: 0.6397 | Acc: 70.70%
  Batch 1250/2258 | Loss: 0.6627 | Acc: 70.76%
  Batch 1300/2258 | Loss: 0.6118 | Acc: 70.74%
  Batch 1350/2258 | Loss: 0.6369 | Acc: 70.78%
  Batch 1400/2258 | Loss: 0.5378 | Acc: 70.83%
  Batch 1450/2258 | Loss: 0.6218 | Acc: 70.88%
  Batch 1500/2258 | Loss: 0.4583 | Acc: 70.91%
  Batch 1550/2258 | Loss: 0.6721 | Acc: 70.93%
  Batch 1600/2258 | Loss: 0.4209 | Acc: 70.93%
  Batch 1650/2258 | Loss: 0.7018 | Acc: 70.93%
  Batch 1700/2258 | Loss: 0.5786 | Acc: 70.95%
  Batch 1750/2258 | Loss: 0.6896 | Acc: 70.97%
  Batch 1800/2258 | Loss: 0.7426 | Acc: 70.93%
  Batch 1850/2258 | Loss: 0.4221 | Acc: 70.98%
  Batch 1900/2258 | Loss: 0.3427 | Acc: 71.08%
  Batch 1950/2258 | Loss: 0.4936 | Acc: 71.15%
  Batch 2000/2258 | Loss: 0.4750 | Acc: 71.16%
  Batch 2050/2258 | Loss: 0.5960 | Acc: 71.22%
  Batch 2100/2258 | Loss: 0.6118 | Acc: 71.18%
  Batch 2150/2258 | Loss: 0.7542 | Acc: 71.21%
  Batch 2200/2258 | Loss: 0.6238 | Acc: 71.20%
  Batch 2250/2258 | Loss: 0.6274 | Acc: 71.23%

Epoch 6 Summary:
  Train Loss: 0.6218 | Train Acc: 71.23%
  Val Loss: 0.6633 | Val Acc: 68.91%
  Time: 230.6s (3.8 min)
  ✓ New best model saved! (Val Acc: 68.91%)
  Validation label distribution:
    Low: True=57, Predicted=44
    Medium-Low: True=444, Predicted=273
    Medium-High: True=4478, Predicted=4474
    High: True=4049, Predicted=4237

Epoch 7/10
------------------------------------------------------------
GPU Memory Allocated: 192.51 MB
GPU Memory Cached: 350.00 MB
  Batch 0/2258 | Loss: 0.6215 | Acc: 78.12%
  Batch 50/2258 | Loss: 0.5349 | Acc: 72.61%
  Batch 100/2258 | Loss: 0.6131 | Acc: 72.00%
  Batch 150/2258 | Loss: 0.4796 | Acc: 72.39%
  Batch 200/2258 | Loss: 0.4685 | Acc: 72.53%
  Batch 250/2258 | Loss: 0.4452 | Acc: 72.46%
  Batch 300/2258 | Loss: 0.5909 | Acc: 72.53%
  Batch 350/2258 | Loss: 0.4964 | Acc: 72.47%
  Batch 400/2258 | Loss: 0.5569 | Acc: 72.57%
  Batch 450/2258 | Loss: 0.4478 | Acc: 72.55%
  Batch 500/2258 | Loss: 0.5966 | Acc: 72.79%
  Batch 550/2258 | Loss: 0.4181 | Acc: 72.88%
  Batch 600/2258 | Loss: 0.6825 | Acc: 73.09%
  Batch 650/2258 | Loss: 0.5540 | Acc: 73.12%
  Batch 700/2258 | Loss: 0.3776 | Acc: 73.23%
  Batch 750/2258 | Loss: 0.5056 | Acc: 73.14%
  Batch 800/2258 | Loss: 0.4044 | Acc: 73.16%
  Batch 850/2258 | Loss: 0.6724 | Acc: 73.14%
  Batch 900/2258 | Loss: 0.5261 | Acc: 73.15%
  Batch 950/2258 | Loss: 0.5159 | Acc: 73.21%
  Batch 1000/2258 | Loss: 0.5795 | Acc: 73.34%
  Batch 1050/2258 | Loss: 0.7624 | Acc: 73.30%
  Batch 1100/2258 | Loss: 0.6882 | Acc: 73.41%
  Batch 1150/2258 | Loss: 0.3927 | Acc: 73.41%
  Batch 1200/2258 | Loss: 0.5760 | Acc: 73.44%
  Batch 1250/2258 | Loss: 0.4514 | Acc: 73.44%
  Batch 1300/2258 | Loss: 0.5345 | Acc: 73.46%
  Batch 1350/2258 | Loss: 0.4169 | Acc: 73.46%
  Batch 1400/2258 | Loss: 0.6523 | Acc: 73.51%
  Batch 1450/2258 | Loss: 0.7070 | Acc: 73.48%
  Batch 1500/2258 | Loss: 0.3497 | Acc: 73.60%
  Batch 1550/2258 | Loss: 0.4415 | Acc: 73.63%
  Batch 1600/2258 | Loss: 0.4320 | Acc: 73.66%
  Batch 1650/2258 | Loss: 0.7573 | Acc: 73.70%
  Batch 1700/2258 | Loss: 0.5758 | Acc: 73.70%
  Batch 1750/2258 | Loss: 0.5448 | Acc: 73.63%
  Batch 1800/2258 | Loss: 0.6323 | Acc: 73.69%
  Batch 1850/2258 | Loss: 0.5996 | Acc: 73.72%
  Batch 1900/2258 | Loss: 0.5787 | Acc: 73.70%
  Batch 1950/2258 | Loss: 0.7393 | Acc: 73.71%
  Batch 2000/2258 | Loss: 0.5649 | Acc: 73.72%
  Batch 2050/2258 | Loss: 0.6725 | Acc: 73.70%
  Batch 2100/2258 | Loss: 0.5443 | Acc: 73.69%
  Batch 2150/2258 | Loss: 0.5262 | Acc: 73.73%
  Batch 2200/2258 | Loss: 0.4280 | Acc: 73.79%
  Batch 2250/2258 | Loss: 0.5365 | Acc: 73.79%

Epoch 7 Summary:
  Train Loss: 0.5722 | Train Acc: 73.81%
  Val Loss: 0.6531 | Val Acc: 69.97%
  Time: 230.4s (3.8 min)
  ✓ New best model saved! (Val Acc: 69.97%)
  Validation label distribution:
    Low: True=57, Predicted=56
    Medium-Low: True=444, Predicted=273
    Medium-High: True=4478, Predicted=4709
    High: True=4049, Predicted=3990

Epoch 8/10
------------------------------------------------------------
GPU Memory Allocated: 193.49 MB
GPU Memory Cached: 336.00 MB
  Batch 0/2258 | Loss: 0.4622 | Acc: 81.25%
  Batch 50/2258 | Loss: 0.4944 | Acc: 76.72%
  Batch 100/2258 | Loss: 0.6941 | Acc: 75.65%
  Batch 150/2258 | Loss: 0.5199 | Acc: 75.43%
  Batch 200/2258 | Loss: 0.5752 | Acc: 75.75%
  Batch 250/2258 | Loss: 0.4995 | Acc: 75.54%
  Batch 300/2258 | Loss: 0.3488 | Acc: 75.76%
  Batch 350/2258 | Loss: 0.4602 | Acc: 76.05%
  Batch 400/2258 | Loss: 0.5824 | Acc: 76.06%
  Batch 450/2258 | Loss: 0.6711 | Acc: 76.04%
  Batch 500/2258 | Loss: 0.5788 | Acc: 76.18%
  Batch 550/2258 | Loss: 0.5321 | Acc: 75.99%
  Batch 600/2258 | Loss: 0.4157 | Acc: 76.07%
  Batch 650/2258 | Loss: 0.4203 | Acc: 76.09%
  Batch 700/2258 | Loss: 0.6453 | Acc: 75.98%
  Batch 750/2258 | Loss: 0.4497 | Acc: 76.01%
  Batch 800/2258 | Loss: 0.6164 | Acc: 76.02%
  Batch 850/2258 | Loss: 0.7879 | Acc: 75.97%
  Batch 900/2258 | Loss: 0.4941 | Acc: 75.97%
  Batch 950/2258 | Loss: 0.4109 | Acc: 75.99%
  Batch 1000/2258 | Loss: 0.3471 | Acc: 75.95%
  Batch 1050/2258 | Loss: 0.4386 | Acc: 75.92%
  Batch 1100/2258 | Loss: 0.6909 | Acc: 75.89%
  Batch 1150/2258 | Loss: 0.3296 | Acc: 75.91%
  Batch 1200/2258 | Loss: 0.5140 | Acc: 75.89%
  Batch 1250/2258 | Loss: 0.5687 | Acc: 75.89%
  Batch 1300/2258 | Loss: 0.5216 | Acc: 75.86%
  Batch 1350/2258 | Loss: 0.5083 | Acc: 75.84%
  Batch 1400/2258 | Loss: 0.5586 | Acc: 75.82%
  Batch 1450/2258 | Loss: 0.4205 | Acc: 75.82%
  Batch 1500/2258 | Loss: 0.5781 | Acc: 75.87%
  Batch 1550/2258 | Loss: 0.4771 | Acc: 75.85%
  Batch 1600/2258 | Loss: 0.6821 | Acc: 75.81%
  Batch 1650/2258 | Loss: 0.5387 | Acc: 75.79%
  Batch 1700/2258 | Loss: 0.4751 | Acc: 75.83%
  Batch 1750/2258 | Loss: 0.3623 | Acc: 75.80%
  Batch 1800/2258 | Loss: 0.4427 | Acc: 75.80%
  Batch 1850/2258 | Loss: 0.4176 | Acc: 75.80%
  Batch 1900/2258 | Loss: 0.6246 | Acc: 75.77%
  Batch 1950/2258 | Loss: 0.5865 | Acc: 75.77%
  Batch 2000/2258 | Loss: 0.5572 | Acc: 75.77%
  Batch 2050/2258 | Loss: 0.6254 | Acc: 75.76%
  Batch 2100/2258 | Loss: 0.5051 | Acc: 75.74%
  Batch 2150/2258 | Loss: 0.6670 | Acc: 75.74%
  Batch 2200/2258 | Loss: 0.5260 | Acc: 75.74%
  Batch 2250/2258 | Loss: 0.4825 | Acc: 75.74%

Epoch 8 Summary:
  Train Loss: 0.5329 | Train Acc: 75.73%
  Val Loss: 0.6515 | Val Acc: 70.29%
  Time: 230.6s (3.8 min)
  ✓ New best model saved! (Val Acc: 70.29%)
  Validation label distribution:
    Low: True=57, Predicted=73
    Medium-Low: True=444, Predicted=413
    Medium-High: True=4478, Predicted=4207
    High: True=4049, Predicted=4335

Epoch 9/10
------------------------------------------------------------
GPU Memory Allocated: 192.51 MB
GPU Memory Cached: 350.00 MB
  Batch 0/2258 | Loss: 0.4526 | Acc: 71.88%
  Batch 50/2258 | Loss: 0.4589 | Acc: 78.49%
  Batch 100/2258 | Loss: 0.4703 | Acc: 78.77%
  Batch 150/2258 | Loss: 0.4792 | Acc: 77.69%
  Batch 200/2258 | Loss: 0.4006 | Acc: 77.83%
  Batch 250/2258 | Loss: 0.4999 | Acc: 78.31%
  Batch 300/2258 | Loss: 0.4263 | Acc: 77.88%
  Batch 350/2258 | Loss: 0.5580 | Acc: 77.94%
  Batch 400/2258 | Loss: 0.4791 | Acc: 77.61%
  Batch 450/2258 | Loss: 0.5028 | Acc: 77.63%
  Batch 500/2258 | Loss: 0.3498 | Acc: 77.66%
  Batch 550/2258 | Loss: 0.4734 | Acc: 77.63%
  Batch 600/2258 | Loss: 0.4041 | Acc: 77.59%
  Batch 650/2258 | Loss: 0.5654 | Acc: 77.59%
  Batch 700/2258 | Loss: 0.5395 | Acc: 77.74%
  Batch 750/2258 | Loss: 0.5483 | Acc: 77.72%
  Batch 800/2258 | Loss: 0.5324 | Acc: 77.65%
  Batch 850/2258 | Loss: 0.4148 | Acc: 77.54%
  Batch 900/2258 | Loss: 0.5508 | Acc: 77.51%
  Batch 950/2258 | Loss: 0.3207 | Acc: 77.54%
  Batch 1000/2258 | Loss: 0.3784 | Acc: 77.60%
  Batch 1050/2258 | Loss: 0.5436 | Acc: 77.54%
  Batch 1100/2258 | Loss: 0.4553 | Acc: 77.55%
  Batch 1150/2258 | Loss: 0.4796 | Acc: 77.44%
  Batch 1200/2258 | Loss: 0.5605 | Acc: 77.50%
  Batch 1250/2258 | Loss: 0.4137 | Acc: 77.53%
  Batch 1300/2258 | Loss: 0.5341 | Acc: 77.61%
  Batch 1350/2258 | Loss: 0.3987 | Acc: 77.61%
  Batch 1400/2258 | Loss: 0.4210 | Acc: 77.61%
  Batch 1450/2258 | Loss: 0.4220 | Acc: 77.60%
  Batch 1500/2258 | Loss: 0.4110 | Acc: 77.56%
  Batch 1550/2258 | Loss: 0.4665 | Acc: 77.59%
  Batch 1600/2258 | Loss: 0.5508 | Acc: 77.65%
  Batch 1650/2258 | Loss: 0.3450 | Acc: 77.67%
  Batch 1700/2258 | Loss: 0.5885 | Acc: 77.65%
  Batch 1750/2258 | Loss: 0.3548 | Acc: 77.65%
  Batch 1800/2258 | Loss: 0.5736 | Acc: 77.67%
  Batch 1850/2258 | Loss: 0.7509 | Acc: 77.70%
  Batch 1900/2258 | Loss: 0.4491 | Acc: 77.73%
  Batch 1950/2258 | Loss: 0.2921 | Acc: 77.76%
  Batch 2000/2258 | Loss: 0.5082 | Acc: 77.74%
  Batch 2050/2258 | Loss: 0.6853 | Acc: 77.71%
  Batch 2100/2258 | Loss: 0.6031 | Acc: 77.74%
  Batch 2150/2258 | Loss: 0.5298 | Acc: 77.75%
  Batch 2200/2258 | Loss: 0.4427 | Acc: 77.72%
  Batch 2250/2258 | Loss: 0.3410 | Acc: 77.77%

Epoch 9 Summary:
  Train Loss: 0.4938 | Train Acc: 77.77%
  Val Loss: 0.6377 | Val Acc: 70.88%
  Time: 231.0s (3.8 min)
  ✓ New best model saved! (Val Acc: 70.88%)
  Validation label distribution:
    Low: True=57, Predicted=61
    Medium-Low: True=444, Predicted=297
    Medium-High: True=4478, Predicted=4540
    High: True=4049, Predicted=4130

Epoch 10/10
------------------------------------------------------------
GPU Memory Allocated: 193.49 MB
GPU Memory Cached: 336.00 MB
  Batch 0/2258 | Loss: 0.4247 | Acc: 84.38%
  Batch 50/2258 | Loss: 0.4785 | Acc: 78.92%
  Batch 100/2258 | Loss: 0.5188 | Acc: 79.61%
  Batch 150/2258 | Loss: 0.2939 | Acc: 79.82%
  Batch 200/2258 | Loss: 0.4077 | Acc: 80.01%
  Batch 250/2258 | Loss: 0.5781 | Acc: 80.04%
  Batch 300/2258 | Loss: 0.4151 | Acc: 80.31%
  Batch 350/2258 | Loss: 0.5334 | Acc: 80.34%
  Batch 400/2258 | Loss: 0.2477 | Acc: 80.10%
  Batch 450/2258 | Loss: 0.6220 | Acc: 80.00%
  Batch 500/2258 | Loss: 0.6051 | Acc: 79.87%
  Batch 550/2258 | Loss: 0.4427 | Acc: 79.75%
  Batch 600/2258 | Loss: 0.4502 | Acc: 79.66%
  Batch 650/2258 | Loss: 0.5583 | Acc: 79.67%
  Batch 700/2258 | Loss: 0.4078 | Acc: 79.55%
  Batch 750/2258 | Loss: 0.4621 | Acc: 79.59%
  Batch 800/2258 | Loss: 0.4975 | Acc: 79.48%
  Batch 850/2258 | Loss: 0.3453 | Acc: 79.50%
  Batch 900/2258 | Loss: 0.4600 | Acc: 79.53%
  Batch 950/2258 | Loss: 0.4427 | Acc: 79.61%
  Batch 1000/2258 | Loss: 0.4666 | Acc: 79.60%
  Batch 1050/2258 | Loss: 0.4782 | Acc: 79.58%
  Batch 1100/2258 | Loss: 0.4964 | Acc: 79.64%
  Batch 1150/2258 | Loss: 0.3968 | Acc: 79.61%
  Batch 1200/2258 | Loss: 0.3804 | Acc: 79.67%
  Batch 1250/2258 | Loss: 0.4665 | Acc: 79.67%
  Batch 1300/2258 | Loss: 0.5542 | Acc: 79.67%
  Batch 1350/2258 | Loss: 0.5392 | Acc: 79.66%
  Batch 1400/2258 | Loss: 0.5298 | Acc: 79.68%
  Batch 1450/2258 | Loss: 0.4718 | Acc: 79.65%
  Batch 1500/2258 | Loss: 0.4156 | Acc: 79.65%
  Batch 1550/2258 | Loss: 0.3341 | Acc: 79.63%
  Batch 1600/2258 | Loss: 0.5107 | Acc: 79.65%
  Batch 1650/2258 | Loss: 0.3403 | Acc: 79.66%
  Batch 1700/2258 | Loss: 0.3195 | Acc: 79.66%
  Batch 1750/2258 | Loss: 0.6007 | Acc: 79.66%
  Batch 1800/2258 | Loss: 0.6843 | Acc: 79.62%
  Batch 1850/2258 | Loss: 0.4103 | Acc: 79.62%
  Batch 1900/2258 | Loss: 0.4001 | Acc: 79.64%
  Batch 1950/2258 | Loss: 0.4689 | Acc: 79.67%
  Batch 2000/2258 | Loss: 0.4733 | Acc: 79.72%
  Batch 2050/2258 | Loss: 0.3934 | Acc: 79.72%
  Batch 2100/2258 | Loss: 0.3985 | Acc: 79.72%
  Batch 2150/2258 | Loss: 0.3084 | Acc: 79.76%
  Batch 2200/2258 | Loss: 0.5909 | Acc: 79.80%
  Batch 2250/2258 | Loss: 0.3961 | Acc: 79.80%

Epoch 10 Summary:
  Train Loss: 0.4576 | Train Acc: 79.78%
  Val Loss: 0.6406 | Val Acc: 72.28%
  Time: 231.1s (3.9 min)
  ✓ New best model saved! (Val Acc: 72.28%)
  Validation label distribution:
    Low: True=57, Predicted=84
    Medium-Low: True=444, Predicted=332
    Medium-High: True=4478, Predicted=4573
    High: True=4049, Predicted=4039

============================================================
Training Complete! Total time: 2551.6s (42.5 min)
Best Validation Accuracy: 72.28%
============================================================

==================================================
Testing Model...
==================================================
Test Loss: 0.6349 | Test Acc: 72.61%

Test Set Analysis:
Test label distribution:
  Low: True=63, Predicted=67, Class Acc=52.4%
  Medium-Low: True=456, Predicted=351, Class Acc=45.8%
  Medium-High: True=4444, Predicted=4536, Class Acc=74.8%
  High: True=4066, Predicted=4075, Class Acc=73.5%

============================================================
Model training complete!
Saved model: best_engagement_model.pth
You can now use this model in app.py for live video prediction
============================================================

Process finished with exit code 0
